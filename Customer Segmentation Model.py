# -*- coding: utf-8 -*-
"""DataMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eBOFH4WUjAcduMr7nBB2NcniCzJcd0Rb

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from sklearn.cluster import KMeans
import pylab as pl
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import normalize
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.cluster import AffinityPropagation

from google.colab import drive
drive.mount('/content/drive')

"""# Load Dataset"""

ds=pd.read_csv("/content/drive/MyDrive/Data Mining/Mall_Customers.csv")

#determine missing values in dataset
ds.isnull().sum()

#First Five rows
print(ds.head())
#Number of Rows
print(len(ds))

ds.describe().transpose()

"""### Plot between Annual Income and Spending Score
### Plot the Points in graph
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)

#Plot the points
IncomePoints = ds['Annual Income (k$)'].values
ScorePoints = ds['Spending Score (1-100)'].values
allPoints = np.array(list(zip(IncomePoints, ScorePoints)))
plt.scatter(IncomePoints, ScorePoints, c='black', s=30)

#select Income and score columns
neededColumns=ds.iloc[:,[3,4]].values
print(neededColumns)

"""# Dendrogram"""

#create dendrogram
dendrogram = sch.dendrogram(sch.linkage(neededColumns,method='ward'))

"""## Agglomertive Clusturing"""

unique_colors=set(dendrogram['color_list'])
print(unique_colors)

optimal_number_of_clusters=len(unique_colors)+
print(optimal_number_of_clusters)

#perform actual clustering
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')

y_hc = hc.fit_predict(neededColumns)

plt.scatter(neededColumns[y_hc==0,0],neededColumns[y_hc==0,1],s=100,c='blue')
plt.scatter(neededColumns[y_hc==1,0],neededColumns[y_hc==1,1],s=100,c='green')
plt.scatter(neededColumns[y_hc==2,0],neededColumns[y_hc==2,1],s=100,c='red')
plt.scatter(neededColumns[y_hc==3,0],neededColumns[y_hc==3,1],s=100,c='yellow')
plt.scatter(neededColumns[y_hc==4,0],neededColumns[y_hc==4,1],s=100,c='black')

"""# DBSCAN"""

from itertools import product

eps_values = np.arange(8,12.75,0.25) # eps values to be investigated
min_samples = np.arange(3,10) # min_samples values to be investigated
DBSCAN_params = list(product(eps_values, min_samples))

X_numerics = ds[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']] # subset with numeric variables only

no_of_clusters = []
sil_score = []

for p in DBSCAN_params:
    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_numerics)
    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))
    sil_score.append(silhouette_score(X_numerics, DBS_clustering.labels_))

tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   
tmp['No_of_clusters'] = no_of_clusters

pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')

fig, ax = plt.subplots(figsize=(12,6))
sns.heatmap(pivot_1, annot=True,annot_kws={"size": 16}, cmap="Reds_r", ax=ax)
ax.set_title('Number of clusters')
plt.show()

DBS_clustering = DBSCAN(eps=12.5, min_samples=4).fit(X_numerics)

DBSCAN_clustered = X_numerics.copy()
DBSCAN_clustered.loc[:,'Cluster'] = DBS_clustering.labels_ # append labels to points

DBSCAN_clust_sizes = DBSCAN_clustered.groupby('Cluster').size().to_frame()
DBSCAN_clust_sizes.columns = ["DBSCAN_size"]
DBSCAN_clust_sizes

outliers = DBSCAN_clustered[DBSCAN_clustered['Cluster']==-1]

fig2, (axes) = plt.subplots(1,2,figsize=(12,5))


sns.scatterplot('Annual Income (k$)', 'Spending Score (1-100)',
                data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1],
                hue='Cluster', ax=axes[0], palette='Set1', legend='full', s=45)

sns.scatterplot('Age', 'Spending Score (1-100)',
                data=DBSCAN_clustered[DBSCAN_clustered['Cluster']!=-1],
                hue='Cluster', palette='Set1', ax=axes[1], legend='full', s=45)

axes[0].scatter(outliers['Annual Income (k$)'], outliers['Spending Score (1-100)'], s=5, label='outliers', c="k")
axes[1].scatter(outliers['Age'], outliers['Spending Score (1-100)'], s=5, label='outliers', c="k")
axes[0].legend()
axes[1].legend()

plt.setp(axes[0].get_legend().get_texts(), fontsize='10')
plt.setp(axes[1].get_legend().get_texts(), fontsize='10')

plt.show()

"""## K-Mean Clustering
#### Elbow Method
"""

#Use K-means
#Calculate the number of centroids needed for best fit
squaredDistance = []
for i in range(1,11):
    km=KMeans(n_clusters=i,init='k-means++', max_iter=300, n_init=10, random_state=0)
    km.fit(neededColumns)
    squaredDistance.append(km.inertia_)

#Plot the result
plt.plot(range(1,11),squaredDistance)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('squaredDistance')
plt.show()

#Calculate silhoutte coefficient
#Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique.
#Its value ranges from -1 to 1.

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans



for n_cluster in range(2, 15):
    kmeans = KMeans(n_clusters=n_cluster).fit(neededColumns)
    label = kmeans.labels_
    sil_coeff = silhouette_score(neededColumns, label, metric='euclidean')
    print("For n_clusters={}, The Silhouette Coefficient is {}".format(n_cluster, sil_coeff))

"""### Fit the Model"""

##Fitting kmeans
km5=KMeans(n_clusters=5,init='k-means++', max_iter=300, n_init=10, random_state=0)
y_means = km5.fit_predict(neededColumns)

#Visualising the clusters for k=5
plt.scatter(neededColumns[y_means==0,0],neededColumns[y_means==0,1],s=30, c='purple',label='Cluster1')
plt.scatter(neededColumns[y_means==1,0],neededColumns[y_means==1,1],s=30, c='blue',label='Cluster2')
plt.scatter(neededColumns[y_means==2,0],neededColumns[y_means==2,1],s=30, c='green',label='Cluster3')
plt.scatter(neededColumns[y_means==3,0],neededColumns[y_means==3,1],s=30, c='cyan',label='Cluster4')
plt.scatter(neededColumns[y_means==4,0],neededColumns[y_means==4,1],s=30, c='gray',label='Cluster5')

plt.scatter(km5.cluster_centers_[:,0], km5.cluster_centers_[:,1],s=200,marker='s', c='red', alpha=0.7, label='Centroids')
plt.title('Customer segmentation')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

"""# Add gender and Age"""

#Calculate n of Males And Females
sns.countplot(x='Gender', data=ds);
plt.title('Distribution of Gender');

labels = ['Female', 'Male']
size = [112, 88]
colors = ['lightgreen', 'orange']
explode = [0, 0.1]

plt.rcParams['figure.figsize'] = (7, 7)
plt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')
plt.title('A pie chart Representing the Gender')
plt.axis('off')
plt.legend()
plt.show()

# Histogram of ages
ds.hist('Age', bins=35);
plt.title('Distribution of Age');
plt.xlabel('Age');

ds['Age'].value_counts().plot.bar(figsize = (9, 9))

"""### Histograms"""

# Histogram of ages by gender
plt.hist('Age', data=ds[ds['Gender'] == 'Male'], alpha=0.5, label='Male');
plt.hist('Age', data=ds[ds['Gender'] == 'Female'], alpha=0.5, label='Female');
plt.title('Distribution of Age by Gender');
plt.xlabel('Age');
plt.legend();

# Histogram of income by gender
plt.hist('Annual Income (k$)', data=ds[ds['Gender'] == 'Male'], alpha=0.5, label='Male');
plt.hist('Annual Income (k$)', data=ds[ds['Gender'] == 'Female'], alpha=0.5, label='Female');
plt.title('Distribution of Income by Gender');
plt.xlabel('Income (Thousands of Dollars)');
plt.legend();

"""## Classify the data Depending on Gender"""

# Create data sets by gender
male_customers = ds[ds['Gender'] == 'Male']
female_customers = ds[ds['Gender'] == 'Female']

# Print the average spending score for men and women
print(male_customers['Spending Score (1-100)'].mean())
print(female_customers['Spending Score (1-100)'].mean())

"""### Visualization Between Gender and (Spending Score - Annual Income)"""

sns.scatterplot('Age', 'Annual Income (k$)', hue='Gender', data=ds);
plt.title('Age to Income, Colored by Gender');

sns.scatterplot('Age', 'Spending Score (1-100)', hue='Gender', data=ds);
plt.title('Age to Spending Score, Colored by Gender');

"""## Heatmap Visualization"""

sns.heatmap(female_customers.corr(), annot=True);
plt.title('Correlation Heatmap - Female');

sns.heatmap(male_customers.corr(), annot=True);
plt.title('Correlation Heatmap - Male');

"""### Graph Visualization"""

sns.scatterplot('Annual Income (k$)', 'Spending Score (1-100)', hue='Gender', data=ds);
plt.title('Annual Income to Spending Score, Colored by Gender');

sns.pairplot(ds,hue='Gender')

import plotly.express as px

fig1 = px.scatter_3d(ds, x="Annual Income (k$)", y="Age", color=kmeans.labels_,
                 size="Spending Score (1-100)", z='Gender')

fig1.update_layout(title="Cluster Representation")
fig1.show()

"""# Conclusion

* Marketing cheaper items to women to see if they purchase more frequently or more volume.

* Marketing more to younger women because their spending score tends to be higher.

* Thinking up new ways to target advertising, pricing, branding, etc. to the older women (older than early 40s) who have lower spending scores.

* Figure out a way to gather more data to build a data set that has more features. The more features, the better understanding of what determines Spending Score. Once Spending Score is better understood, we can understand what factors will lead to increasing Spending Score, thus lead to greater profits.
"""